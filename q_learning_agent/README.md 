# Q-Learning Agent (Q2)

This folder contains the **Q-Learning agent** implementation used in Assignment 3 (Q2).  
It includes the Q-table learning algorithm, compact state encoding, training commands, and evaluation notes.

---

# 1. Summary

### ‚úî Algorithm  
- **Tabular Q-learning**  
- **Epsilon-greedy** exploration  
- **Learning rate (Œ±)** decay optional  
- **Discount factor (Œ≥)** tuned through experiments  
- Trains on full episodes, updates per transition

### ‚úî Final Hyperparameters (Chosen Through Search)
From experiments:
epsilon = 0.12
alpha   = 0.22
gamma   = 0.90   # chosen for stability on evaluation server

These values gave high performance across small, medium, and large layouts under the assignment‚Äôs restricted training budgets.

---

# 2. State Representation

A compact state encoding was used to keep the table small and converge quickly.

It includes:
- Pac-Man position (tile coordinates)
- Ghost danger flag (within Manhattan distance 2)
- Food mask (4-bit)
- Wall mask (4-bit)

A detailed explanation is provided in:  
üìÑ **`state_encoding.md`**

---

# 3. File Overview

| File | Purpose |
|------|---------|
| `q2_agent.py` | Main Q-learning agent implementation |
| `state_encoding.md` | Explanation of the encoding and reasoning |
| `README.md` | This file |

---

# 4. Training & Evaluation Commands

### **Run on a small Q-learning layout**
```bash
python pacman.py -l layouts/QL_smallMaze_1.lay -p Q2Agent \
  -a epsilon=0.12,alpha=0.22,gamma=0.90 \
  -x 1000 -n 1040

python pacman.py -l layouts/QL_mediumMaze_2.lay -p Q2Agent \
  -x 2000 -n 2040 \
  -a epsilon=0.12,alpha=0.22,gamma=0.90

python pacman.py -l layouts/QL_largeMaze_3.lay -p Q2Agent \
  -x 4000 -n 4040 \
  -a epsilon=0.12,alpha=0.22,gamma=0.90

Where:
	‚Ä¢	-x K = number of training episodes
	‚Ä¢	-n N = total episodes (training + evaluation)
	‚Ä¢	This follows the assignment‚Äôs RL training protocol.


### How Q-learning Updates Work

The agent updates the Q-table using:

[
Q(s, a) ‚Üê Q(s, a) + \alpha \big[r + \gamma \max_{a‚Äô} Q(s‚Äô, a‚Äô) - Q(s, a)\big]
]
	‚Ä¢	Updates occur after each action.
	‚Ä¢	Table is indexed by:
(pacman_x, pacman_y, ghost_flag, food_mask, wall_mask).

### Strengths of This Approach

‚úî Fast convergence
Compact state = faster learning with limited training episodes.

‚úî Robust behaviour
Generalises across layouts due to relative, not absolute, features.

‚úî Reliable on evaluation server
Selected hyperparameters minimise variance from server randomness.


### Limitations
	‚Ä¢	No long-range ghost lookahead ‚Üí sometimes overeager chasing food in open areas.
	‚Ä¢	No capsule awareness.
	‚Ä¢	Purely reactive (no planning/rollouts).

These were intentional to stay within the assignment constraints and keep table size small.

### Reproducibility

All hyperparameters, training instructions, and experimental results appear in the main project report (docs/report.pdf) and are reproducible with the commands provided above.